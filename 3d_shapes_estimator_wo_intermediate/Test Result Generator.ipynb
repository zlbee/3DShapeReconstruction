{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Test Result Generator.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPR3B5Rmo5XeJx+hYBlO/9f"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"O-VrIDVYIXBn"},"outputs":[],"source":["import os, sys\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","project_path = '/content/gdrive/MyDrive/master/3d_shapes_estimator'\n","sys.path.append(project_path)"]},{"cell_type":"code","source":["# reference: https://keras.io/examples/vision/depth_estimation/\n","class DataLoader(tf.keras.utils.Sequence):\n","  def __init__(self, data, batch_size=15, shuffle=False):\n","    \"\"\"\n","    Initialization\n","    \"\"\"\n","    self.shuffle = shuffle\n","    self.data = data\n","    self.x1_file = self.data[0]\n","    self.x2_file = self.data[1]\n","    self.y_file = self.data[2]\n","    self.x0_file = self.data[3]\n","    self.indices = list(range(self.x1_file.len()))\n","    self.len_indices = len(self.indices)\n","    self.batch_size = batch_size\n","    self.on_epoch_end()\n","\n","  def __len__(self):\n","    if IS_SAMPLE == True:\n","      return int(np.ceil(self.len_indices / self.batch_size / SAMPLE_RATE))\n","    else:\n","      return int(np.ceil(self.len_indices / self.batch_size))\n","\n","  def __getitem__(self, index):\n","    # modify batch size of last batch\n","    batch_size = self.batch_size\n","    if (index + 1) * self.batch_size > len(self.indices):\n","      batch_size = len(self.indices) - index * self.batch_size\n","    # Generate one batch of data\n","    # Generate indices of the batch\n","    index = self.indices[index * batch_size : (index + 1) * batch_size]\n","    # Find list of IDs\n","    x1, x2, y, x0 = self.load_batch(index)\n","    return x1, x2, y, x0\n","\n","\n","  def on_epoch_end(self):\n","    \"\"\"\n","    Updates indexes after each epoch\n","    \"\"\"\n","    self.index = np.arange(len(self.indices))\n","    if self.shuffle == True:\n","        np.random.shuffle(self.index)\n","\n","  def load_batch(self, batch):\n","    \"\"\"\n","    Load one batch of data.\n","    \"\"\"\n","    x1 = self.x1_file[batch[0]:batch[-1]+1]\n","    x2 = self.x2_file[batch[0]:batch[-1]+1]\n","    y = (self.y_file[batch[0]:batch[-1]+1]).astype(np.float16)\n","    x0 = self.x0_file[batch[0]:batch[-1]+1]\n","    return x1, x2, y, x0"],"metadata":{"id":"X0WakOz2Jrm0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import h5py"],"metadata":{"id":"NpmBSFfRJdxf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hdf5_path = '/content/gdrive/MyDrive/master/Data/half_data_shuffled.hdf5'\n","train_dataset_file = (h5py.File(hdf5_path)['train_normal'], h5py.File(hdf5_path)['train_depth'], h5py.File(hdf5_path)['train_shape'], h5py.File(hdf5_path)['train_raw_image'])\n","test_dataset_file = (h5py.File(hdf5_path)['test_normal'], h5py.File(hdf5_path)['test_depth'], h5py.File(hdf5_path)['test_shape'], h5py.File(hdf5_path)['test_raw_image'])\n","validation_dataset_file = (h5py.File(hdf5_path)['validation_normal'], h5py.File(hdf5_path)['validation_depth'], h5py.File(hdf5_path)['validation_shape'], h5py.File(hdf5_path)['validation_raw_image'])"],"metadata":{"id":"BacU1LkrJtkH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metric_hist = []\n","\n","# # load model\n","# loaded_generator = tf.saved_model.load(OUTPUT_PATH+'gen_weights03.ckpt')\n","# loaded_discriminator = tf.saved_model.load(OUTPUT_PATH+'disc_weights03.ckpt')\n","# # loaded_generator = tf.keras.models.load_model(OUTPUT_PATH+'gen_weights')\n","# # loaded_discriminator = tf.keras.models.load_model(OUTPUT_PATH+'disc_weights')\n","\n","generator.load_weights(OUTPUT_PATH+'gen_weights%02d.ckpt' % curr_epoch)\n","discriminator.load_weights(OUTPUT_PATH+'disc_weights%02d.ckpt' % curr_epoch)\n","loaded_generator = generator\n","loaded_discriminator = discriminator\n","\n","loaded_model = ShapeEstimator(generator=loaded_generator, discriminator=loaded_discriminator, gp_weight=GP_WEIGHT)\n","loaded_model.compile(gen_optimizer=gen_optimizer, disc_optimizer=disc_optimizer, metric=[tf.keras.metrics.RootMeanSquaredError(), earth_mover_loss])\n","\n","# loaded_model = model\n","\n","# test batches\n","count = 0\n","for step, batch in enumerate(test_loader):\n","  # save original image\n","  if step % 10 == 0:\n","    try:\n","      os.makedirs(OUTPUT_PATH+'predicted_shapes_test')\n","    except FileExistsError:\n","      pass\n","    with open(OUTPUT_PATH+'predicted_shapes_test/real_raw_image%02d.npz' % count, 'wb+') as f:\n","      np.save(f, batch[3])\n","    f.close()\n","    with open(OUTPUT_PATH+'predicted_shapes_test/real_normal_image%02d.npz' % count, 'wb+') as f:\n","      np.save(f, batch[0])\n","    f.close()\n","    with open(OUTPUT_PATH+'predicted_shapes_test/real_depth_image%02d.npz' % count, 'wb+') as f:\n","      np.save(f, batch[1])\n","    f.close()\n","    with open(OUTPUT_PATH+'predicted_shapes_test/predicted_normal_image%02d.npz' % count, 'wb+') as f:\n","      np.save(f, sketch_estimator.predict(batch[3]))\n","    f.close()\n","    with open(OUTPUT_PATH+'predicted_shapes_test/predicted_depth_image%02d.npz' % count, 'wb+') as f:\n","      np.save(f, sketch_estimator_depth.predict(batch[3]))\n","    f.close()\n","\n","  # predict sketches\n","  # batch = list(batch)\n","  # batch[0] = sketch_estimator.predict(batch[0])\n","  # batch = tuple(batch)\n","\n","  # concatenate sketches\n","  concatenated = np.concatenate((batch[0], batch[1]), axis=3)\n","  batch = (concatenated, batch[2])\n","\n","  metric, shape_pred = loaded_model.test_step(batch)\n","  metric_hist.append(metric)\n","\n","  # save sample predicted shapes and real shapes\n","  if step % 10 == 0:\n","    try:\n","      os.makedirs(OUTPUT_PATH+'predicted_shapes_test')\n","    except FileExistsError:\n","      pass\n","    with open(OUTPUT_PATH+'predicted_shapes_test/predicted_shape%02d.npz' % count, 'wb+') as f:\n","      np.save(f, shape_pred)\n","    f.close()\n","    with open(OUTPUT_PATH+'predicted_shapes_test/real_shape%02d.npz' % count, 'wb+') as f:\n","      np.save(f, batch[1])\n","    f.close()\n","    # with open(OUTPUT_PATH+'predicted_shapes_test/predicted_image%02d.npz' % count, 'wb+') as f:\n","    #   np.save(f, batch[0])\n","    # f.close()\n","    print('Shape saved at step %02d' % step)\n","    count += 1\n","avg_metric_MAE = sum(metric_hist[0]) / len(metric_hist[0])\n","avg_metric_EM = sum(metric_hist[1]) / len(metric_hist[1])\n","print('Average metric MAE: ' + str(avg_metric_MAE))\n","print('Average metric EM: ' + str(avg_metric_EM))"],"metadata":{"id":"_ianIuWEKlsJ"},"execution_count":null,"outputs":[]}]}